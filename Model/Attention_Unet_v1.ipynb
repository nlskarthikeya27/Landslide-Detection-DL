{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Landslide mapping on SAR data by Attention U-Net\n",
    "*Authors: Lorenzo Nava, Kushanav Bhuyan, and Sansar Raj Meena*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
    "from losses import dice_loss\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from helper import *\n",
    "import pandas as pd\n",
    "\n",
    "# Library with segmentation metrics\n",
    "import segmentation_models as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_train = np.load(f'data/Arrays/Arrays_BAA_S_Ascending/trainX.npy')\n",
    "Y_train = np.load(f'data/Arrays/Arrays_BAA_S_Ascending/trainY.npy')\n",
    "X_test = np.load(f'data/Arrays/Arrays_BAA_S_Ascending/testX.npy')\n",
    "Y_test = np.load(f'data/Arrays/Arrays_BAA_S_Ascending/testY.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the evaluation metrics Precision, Recall, F1-Score, IoU\n",
    "metrics = [sm.metrics.Precision(threshold=0.5),sm.metrics.Recall(threshold=0.5),sm.metrics.FScore(threshold=0.5,beta=1),sm.metrics.IOUScore(threshold=0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model training - Results are saved in a .csv file\n",
    "\n",
    "# Size of the tiles\n",
    "size = 64\n",
    "# Image bands\n",
    "img_bands = X_train.shape[3]\n",
    "# Sampling method\n",
    "sampling = \"no_overlap\"\n",
    "# Number of filters \n",
    "filters = [4, 8, 16, 32]\n",
    "# Learning rates\n",
    "lr = [10e-3, 5e-4, 10e-4, 5e-5, 10e-5]\n",
    "# Batch sizes \n",
    "batch_size = [4, 8, 16, 32]\n",
    "# Epochs\n",
    "epochs = 200\n",
    "\n",
    "# Dictionary that will save the results\n",
    "dic = {}\n",
    "\n",
    "# Hyperparameters\n",
    "dic[\"model\"] = []\n",
    "dic[\"batch_size\"] = []\n",
    "dic[\"learning_rate\"] = []\n",
    "dic[\"filters\"] = []\n",
    "\n",
    "# test area 1\n",
    "dic[\"precision_area\"] = []\n",
    "dic[\"recall_area\"] = []\n",
    "dic[\"f1_score_area\"] = []\n",
    "dic[\"iou_score_area\"] = []\n",
    "\n",
    "# loop over all the filters in the filter list\n",
    "for fiilter in filters:\n",
    "    # loop over the learning rates\n",
    "    for learning_rate in lr:\n",
    "        # loop over all batch sizes in batch_size list\n",
    "        for batch in batch_size:\n",
    "            def attn_unet(lr,filtersFirstLayer, pretrained_weights = None,input_size = (size,size,img_bands)):   \n",
    "                inputs = Input(shape=input_size)\n",
    "                conv1 = UnetConv2D(inputs, filtersFirstLayer, is_batchnorm=True, name='conv1')\n",
    "                pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "                conv2 = UnetConv2D(pool1, filtersFirstLayer, is_batchnorm=True, name='conv2')\n",
    "                pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "                conv3 = UnetConv2D(pool2, filtersFirstLayer*2, is_batchnorm=True, name='conv3')\n",
    "                #conv3 = Dropout(0.2,name='drop_conv3')(conv3)\n",
    "                pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "                conv4 = UnetConv2D(pool3, filtersFirstLayer*2, is_batchnorm=True, name='conv4')\n",
    "                #conv4 = Dropout(0.2, name='drop_conv4')(conv4)\n",
    "                pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "                center = UnetConv2D(pool4, filtersFirstLayer*4, is_batchnorm=True, name='center')\n",
    "\n",
    "                g1 = UnetGatingSignal(center, is_batchnorm=True, name='g1')\n",
    "                attn1 = AttnGatingBlock(conv4, g1, filtersFirstLayer*4, '_1')\n",
    "                up1 = concatenate([Conv2DTranspose(filtersFirstLayer, (3,3), strides=(2,2), padding='same', activation='relu', kernel_initializer=kinit)(center), attn1], name='up1')\n",
    "\n",
    "                g2 = UnetGatingSignal(up1, is_batchnorm=True, name='g2')\n",
    "                attn2 = AttnGatingBlock(conv3, g2, filtersFirstLayer*2, '_2')\n",
    "                up2 = concatenate([Conv2DTranspose(filtersFirstLayer*2, (3,3), strides=(2,2), padding='same', activation='relu', kernel_initializer=kinit)(up1), attn2], name='up2')\n",
    "\n",
    "                g3 = UnetGatingSignal(up1, is_batchnorm=True, name='g3')\n",
    "                attn3 = AttnGatingBlock(conv2, g3, filtersFirstLayer, '_3')\n",
    "                up3 = concatenate([Conv2DTranspose(filtersFirstLayer, (3,3), strides=(2,2), padding='same', activation='relu', kernel_initializer=kinit)(up2), attn3], name='up3')\n",
    "\n",
    "                up4 = concatenate([Conv2DTranspose(filtersFirstLayer, (3,3), strides=(2,2), padding='same', activation='relu', kernel_initializer=kinit)(up3), conv1], name='up4')\n",
    "                conv10 = Conv2D(1, (1, 1), activation='sigmoid',  kernel_initializer=kinit, name='final')(up4)\n",
    "\n",
    "                model = Model(inputs, conv10)\n",
    "\n",
    "                model.compile(optimizer = Adam(lr = lr), loss = dice_loss, metrics = metrics)\n",
    "                model.summary()\n",
    "\n",
    "                if(pretrained_weights):\n",
    "                    model.load_weights(pretrained_weights)\n",
    "\n",
    "                return model\n",
    "\n",
    "\n",
    "\n",
    "            # Load the model\n",
    "            model = attn_unet(filtersFirstLayer= fiilter, lr = learning_rate, input_size = (size,size,img_bands))\n",
    "            # Stop the training if the validation loss does not decrease after 30 epochs\n",
    "            early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss', # what is the metric to measure\n",
    "                              patience = 30, # how many epochs to continue running the model after seeing an increase in val_loss\n",
    "                              restore_best_weights = True) # update the mod\n",
    "            # Save the models only when validation loss decrease\n",
    "            model_checkpoint = tf.keras.callbacks.ModelCheckpoint(f'data/model/att_unet/weights/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}.hdf5', monitor='val_loss', mode='min',verbose=1, save_best_only=True,save_weights_only = True)\n",
    "            print(fiilter, learning_rate,batch)\n",
    "            # fit the model 20% of the dataset was used as validation\n",
    "            history = model.fit(X_train,Y_train,batch_size = batch,epochs=epochs,validation_split=0.2,callbacks = [model_checkpoint, early_stop])\n",
    "\n",
    "            # summarize history for iou score\n",
    "            plt.plot(history.history['f1-score'])\n",
    "            plt.plot(history.history['val_f1-score'])\n",
    "            plt.title('model f1-score')\n",
    "            plt.ylabel('f1-score')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'validation'], loc='upper left')\n",
    "            # save plots \n",
    "            plt.savefig(f\"data/model/att_unet/plots/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}_iou_score.png\")\n",
    "            plt.show()\n",
    "            # summarize history for loss\n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.title('model loss')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'validation'], loc='upper left')\n",
    "            plt.savefig(f\"data/model/att_unet/plots/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}_val_loss.png\")\n",
    "            plt.show()\n",
    "            \n",
    "            # load unet to evaluate the test data\n",
    "            attn_unet = attn_unet(filtersFirstLayer= fiilter, lr = learning_rate,input_size=(size,size,img_bands))\n",
    "            # load the last saved weight from the training\n",
    "            attn_unet.load_weights(f\"data/model/att_unet/weights/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}.hdf5\")\n",
    "            \n",
    "           # Evaluate the model\n",
    "            res_1= attn_unet.evaluate(X_test,Y_test)\n",
    "\n",
    "\n",
    "            # save results on the dictionary\n",
    "            dic[\"model\"].append(\"Attention_Unet\")\n",
    "            dic[\"batch_size\"].append(batch)\n",
    "            dic[\"learning_rate\"].append(learning_rate)\n",
    "            dic[\"filters\"].append(fiilter)\n",
    "            dic[\"precision_area\"].append(res_1[1])\n",
    "            dic[\"recall_area\"].append(res_1[2])\n",
    "            dic[\"f1_score_area\"].append(res_1[3])\n",
    "            dic[\"iou_score_area\"].append(res_1[4])\n",
    "            \n",
    "\n",
    "\n",
    "            # Convert results to a dataframe\n",
    "            results = pd.DataFrame(dic)\n",
    "            # Export as csv\n",
    "            results.to_csv(f'data/model/att_unet/results/results_Att_Unet.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the best model based on the best performances on the test set (check CSV file)\n",
    "# Loading the model weights\n",
    "attn_unet_best = attn_unet(filtersFirstLayer= 4,lr = 0.001,input_size=(size,size,img_bands))\n",
    "attn_unet_best.load_weights(\"data/model/att_unet/weights/unet_no_overlap_size_64_filters_4_batch_size_4_lr_0.001.hdf5\")\n",
    "\n",
    "# Plot predictions on test set\n",
    "for i in range(X_test.shape[0]):\n",
    "    preds_train_1 = attn_unet_best.predict(np.expand_dims(X_test[i],axis = 0), verbose=1)\n",
    "    # It's possible to change the 0.5 threshold to improve the results;\n",
    "    preds_train_t1 = (preds_train_1 > 0.9).astype(np.uint8)\n",
    "    f, axarr = plt.subplots(1,3,figsize=(10,10))\n",
    "    axarr[0].imshow(X_test[i][:,:,0])\n",
    "    axarr[1].imshow(np.squeeze(preds_train_t1))\n",
    "    axarr[2].imshow(np.squeeze(Y_test[i]))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
